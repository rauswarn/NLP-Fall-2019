{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "_Gc0ygdFMoB-",
        "5_AZQpj1MvAo",
        "ne4aOKuXM9Ec"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2RAuSIXO9Ds",
        "colab_type": "text"
      },
      "source": [
        "**Text Preprocessing**\n",
        "\n",
        "\n",
        "*   Tokenization-- Word and sentence\n",
        "*   Stopwords removal\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "*   Frequency distribution\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbM6mAcrQa57",
        "colab_type": "text"
      },
      "source": [
        "**NLTK** (Natural Language ToolKit): Processing Natural Language Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNwRMkUOOwEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text='''In the early days, many language-processing systems were designed by hand-\n",
        "        coding a set of rules,[9],[10] e.g. by writing grammars or devising heuristic \n",
        "        rules for stemming. However, this is rarely robust to natural language variation.\n",
        "        Since the so-called \"statistical revolution\"[11][12] in the late 1980s and mid \n",
        "        1990s, much natural language processing research has relied heavily on machine \n",
        "        learning.The machine-learning paradigm calls instead for using statistical inference \n",
        "        to automatically learn such rules through the analysis of large corpora of typical\n",
        "        real-world examples (a corpus (plural, \"corpora\") is a set of documents, possibly \n",
        "        with human or computer annotations).Many different classes of machine-learning \n",
        "        algorithms have been applied to natural-language-processing tasks. These \n",
        "        algorithms take as input a large set of \"features\" that are generated from the \n",
        "        input data. Some of the earliest-used algorithms, such as decision trees, produced \n",
        "        systems of hard if-then rules similar to the systems of hand-written rules that were \n",
        "        then common. Increasingly, however, research has focused on statistical models, \n",
        "        which make soft, probabilistic decisions based on attaching real-valued weights \n",
        "        to each input feature. Such models have the advantage that they can express the \n",
        "        relative certainty of many different possible answers rather than only one, producing\n",
        "        more reliable results when such a model is included as a component of a larger system.'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiXJDGDdMdeP",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4I_ootj3J8G",
        "colab_type": "code",
        "outputId": "af83ed99-efc4-4d2b-e0dd-909ffae988f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN4_78kt2xsY",
        "colab_type": "code",
        "outputId": "e3a74ef6-917d-4ceb-d37c-cd21fcfabab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# word tokenization\n",
        "\n",
        "tokens=nltk.word_tokenize(text)\n",
        "print(type(tokens), '\\nNumber of Tokens:', len(tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> \n",
            "Number of Tokens: 255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIp9AilV2zEf",
        "colab_type": "code",
        "outputId": "d5b6e84c-7667-4a0d-b43a-ec770da0703a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "tokens[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'the',\n",
              " 'early',\n",
              " 'days',\n",
              " ',',\n",
              " 'many',\n",
              " 'language-processing',\n",
              " 'systems',\n",
              " 'were',\n",
              " 'designed']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wr_lHIs3r6T",
        "colab_type": "code",
        "outputId": "a91f2ce6-c9b7-4fed-c6f6-76678e4d2549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokens[-10:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['is', 'included', 'as', 'a', 'component', 'of', 'a', 'larger', 'system', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUmuKN7h4LSM",
        "colab_type": "code",
        "outputId": "682b03ed-0cd1-4193-9d5f-da341f7d9fb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# sentence tokenization\n",
        "\n",
        "sentences=nltk.sent_tokenize(text)\n",
        "print(type(sentences), '\\nNumber of Sentences:', len(sentences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> \n",
            "Number of Sentences: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAgjNHGC4yfr",
        "colab_type": "code",
        "outputId": "f7efdeac-fabf-41e6-d378-d3dde7ba07b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "sentences[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In the early days, many language-processing systems were designed by hand-\\n        coding a set of rules,[9],[10] e.g.',\n",
              " 'by writing grammars or devising heuristic \\n        rules for stemming.',\n",
              " 'However, this is rarely robust to natural language variation.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gc0ygdFMoB-",
        "colab_type": "text"
      },
      "source": [
        "# Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW6OEpdn42CU",
        "colab_type": "code",
        "outputId": "fc304e17-fe01-47ca-df1c-fb3e474a2b30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# stopwords removal\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopword = set(stopwords.words('english'))\n",
        "stopword"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsuiji0O5k-r",
        "colab_type": "code",
        "outputId": "c951bd93-93a3-4e15-93ed-5fbec30b2870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "no_stop=''\n",
        "for i in range(len(tokens)):\n",
        "  if tokens[i] not in stopword:\n",
        "    no_stop+=tokens[i]+' '\n",
        "  else:\n",
        "    print(tokens[i])\n",
        "no_stop=no_stop.strip()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the\n",
            "were\n",
            "by\n",
            "a\n",
            "of\n",
            "by\n",
            "or\n",
            "for\n",
            "this\n",
            "is\n",
            "to\n",
            "the\n",
            "in\n",
            "the\n",
            "and\n",
            "has\n",
            "on\n",
            "for\n",
            "to\n",
            "such\n",
            "through\n",
            "the\n",
            "of\n",
            "of\n",
            "a\n",
            "is\n",
            "a\n",
            "of\n",
            "with\n",
            "or\n",
            "of\n",
            "have\n",
            "been\n",
            "to\n",
            "as\n",
            "a\n",
            "of\n",
            "that\n",
            "are\n",
            "from\n",
            "the\n",
            "of\n",
            "the\n",
            "such\n",
            "as\n",
            "of\n",
            "to\n",
            "the\n",
            "of\n",
            "that\n",
            "were\n",
            "then\n",
            "has\n",
            "on\n",
            "which\n",
            "on\n",
            "to\n",
            "each\n",
            "have\n",
            "the\n",
            "that\n",
            "they\n",
            "can\n",
            "the\n",
            "of\n",
            "than\n",
            "only\n",
            "more\n",
            "when\n",
            "such\n",
            "a\n",
            "is\n",
            "as\n",
            "a\n",
            "of\n",
            "a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJjLxgUg6TUH",
        "colab_type": "code",
        "outputId": "9e6d45a2-a635-4c48-bfc5-45da623bafa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Length of text with stopwords:', len(text))\n",
        "print('Length of text with no stop_words:', len(no_stop))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text with stopwords: 1603\n",
            "Length of text with no stop_words: 1218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_AZQpj1MvAo",
        "colab_type": "text"
      },
      "source": [
        "# Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH9NtFuu6Ywo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stemming\n",
        "# reduce to root word\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stem_token=[]\n",
        "for token in tokens:\n",
        "    stem_token.append(stemmer.stem(token))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zt-fcoE-pp2",
        "colab_type": "code",
        "outputId": "22c818fa-3d97-48f3-afbd-51d8bb4b7efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(len(tokens)):\n",
        "  print(tokens[i],'-->',stem_token[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In --> In\n",
            "the --> the\n",
            "early --> earli\n",
            "days --> day\n",
            ", --> ,\n",
            "many --> mani\n",
            "language-processing --> language-process\n",
            "systems --> system\n",
            "were --> were\n",
            "designed --> design\n",
            "by --> by\n",
            "hand- --> hand-\n",
            "coding --> code\n",
            "a --> a\n",
            "set --> set\n",
            "of --> of\n",
            "rules --> rule\n",
            ", --> ,\n",
            "[ --> [\n",
            "9 --> 9\n",
            "] --> ]\n",
            ", --> ,\n",
            "[ --> [\n",
            "10 --> 10\n",
            "] --> ]\n",
            "e.g --> e.g\n",
            ". --> .\n",
            "by --> by\n",
            "writing --> write\n",
            "grammars --> grammar\n",
            "or --> or\n",
            "devising --> devis\n",
            "heuristic --> heurist\n",
            "rules --> rule\n",
            "for --> for\n",
            "stemming --> stem\n",
            ". --> .\n",
            "However --> howev\n",
            ", --> ,\n",
            "this --> thi\n",
            "is --> is\n",
            "rarely --> rare\n",
            "robust --> robust\n",
            "to --> to\n",
            "natural --> natur\n",
            "language --> languag\n",
            "variation --> variat\n",
            ". --> .\n",
            "Since --> sinc\n",
            "the --> the\n",
            "so-called --> so-cal\n",
            "`` --> ``\n",
            "statistical --> statist\n",
            "revolution --> revolut\n",
            "'' --> ''\n",
            "[ --> [\n",
            "11 --> 11\n",
            "] --> ]\n",
            "[ --> [\n",
            "12 --> 12\n",
            "] --> ]\n",
            "in --> in\n",
            "the --> the\n",
            "late --> late\n",
            "1980s --> 1980\n",
            "and --> and\n",
            "mid --> mid\n",
            "1990s --> 1990\n",
            ", --> ,\n",
            "much --> much\n",
            "natural --> natur\n",
            "language --> languag\n",
            "processing --> process\n",
            "research --> research\n",
            "has --> ha\n",
            "relied --> reli\n",
            "heavily --> heavili\n",
            "on --> on\n",
            "machine --> machin\n",
            "learning.The --> learning.th\n",
            "machine-learning --> machine-learn\n",
            "paradigm --> paradigm\n",
            "calls --> call\n",
            "instead --> instead\n",
            "for --> for\n",
            "using --> use\n",
            "statistical --> statist\n",
            "inference --> infer\n",
            "to --> to\n",
            "automatically --> automat\n",
            "learn --> learn\n",
            "such --> such\n",
            "rules --> rule\n",
            "through --> through\n",
            "the --> the\n",
            "analysis --> analysi\n",
            "of --> of\n",
            "large --> larg\n",
            "corpora --> corpora\n",
            "of --> of\n",
            "typical --> typic\n",
            "real-world --> real-world\n",
            "examples --> exampl\n",
            "( --> (\n",
            "a --> a\n",
            "corpus --> corpu\n",
            "( --> (\n",
            "plural --> plural\n",
            ", --> ,\n",
            "`` --> ``\n",
            "corpora --> corpora\n",
            "'' --> ''\n",
            ") --> )\n",
            "is --> is\n",
            "a --> a\n",
            "set --> set\n",
            "of --> of\n",
            "documents --> document\n",
            ", --> ,\n",
            "possibly --> possibl\n",
            "with --> with\n",
            "human --> human\n",
            "or --> or\n",
            "computer --> comput\n",
            "annotations --> annot\n",
            ") --> )\n",
            ".Many --> .mani\n",
            "different --> differ\n",
            "classes --> class\n",
            "of --> of\n",
            "machine-learning --> machine-learn\n",
            "algorithms --> algorithm\n",
            "have --> have\n",
            "been --> been\n",
            "applied --> appli\n",
            "to --> to\n",
            "natural-language-processing --> natural-language-process\n",
            "tasks --> task\n",
            ". --> .\n",
            "These --> these\n",
            "algorithms --> algorithm\n",
            "take --> take\n",
            "as --> as\n",
            "input --> input\n",
            "a --> a\n",
            "large --> larg\n",
            "set --> set\n",
            "of --> of\n",
            "`` --> ``\n",
            "features --> featur\n",
            "'' --> ''\n",
            "that --> that\n",
            "are --> are\n",
            "generated --> gener\n",
            "from --> from\n",
            "the --> the\n",
            "input --> input\n",
            "data --> data\n",
            ". --> .\n",
            "Some --> some\n",
            "of --> of\n",
            "the --> the\n",
            "earliest-used --> earliest-us\n",
            "algorithms --> algorithm\n",
            ", --> ,\n",
            "such --> such\n",
            "as --> as\n",
            "decision --> decis\n",
            "trees --> tree\n",
            ", --> ,\n",
            "produced --> produc\n",
            "systems --> system\n",
            "of --> of\n",
            "hard --> hard\n",
            "if-then --> if-then\n",
            "rules --> rule\n",
            "similar --> similar\n",
            "to --> to\n",
            "the --> the\n",
            "systems --> system\n",
            "of --> of\n",
            "hand-written --> hand-written\n",
            "rules --> rule\n",
            "that --> that\n",
            "were --> were\n",
            "then --> then\n",
            "common --> common\n",
            ". --> .\n",
            "Increasingly --> increasingli\n",
            ", --> ,\n",
            "however --> howev\n",
            ", --> ,\n",
            "research --> research\n",
            "has --> ha\n",
            "focused --> focus\n",
            "on --> on\n",
            "statistical --> statist\n",
            "models --> model\n",
            ", --> ,\n",
            "which --> which\n",
            "make --> make\n",
            "soft --> soft\n",
            ", --> ,\n",
            "probabilistic --> probabilist\n",
            "decisions --> decis\n",
            "based --> base\n",
            "on --> on\n",
            "attaching --> attach\n",
            "real-valued --> real-valu\n",
            "weights --> weight\n",
            "to --> to\n",
            "each --> each\n",
            "input --> input\n",
            "feature --> featur\n",
            ". --> .\n",
            "Such --> such\n",
            "models --> model\n",
            "have --> have\n",
            "the --> the\n",
            "advantage --> advantag\n",
            "that --> that\n",
            "they --> they\n",
            "can --> can\n",
            "express --> express\n",
            "the --> the\n",
            "relative --> rel\n",
            "certainty --> certainti\n",
            "of --> of\n",
            "many --> mani\n",
            "different --> differ\n",
            "possible --> possibl\n",
            "answers --> answer\n",
            "rather --> rather\n",
            "than --> than\n",
            "only --> onli\n",
            "one --> one\n",
            ", --> ,\n",
            "producing --> produc\n",
            "more --> more\n",
            "reliable --> reliabl\n",
            "results --> result\n",
            "when --> when\n",
            "such --> such\n",
            "a --> a\n",
            "model --> model\n",
            "is --> is\n",
            "included --> includ\n",
            "as --> as\n",
            "a --> a\n",
            "component --> compon\n",
            "of --> of\n",
            "a --> a\n",
            "larger --> larger\n",
            "system --> system\n",
            ". --> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq8s_7TnM2T-",
        "colab_type": "text"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89QRKk2H-zoZ",
        "colab_type": "code",
        "outputId": "e954e59c-589c-4f55-b0df-ff005c21c0c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# lemmatization\n",
        "# converts to base word. \n",
        "\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemma_token=[]\n",
        "for token in tokens:\n",
        "    lemma_token.append(lemmatizer.lemmatize(token))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meT0rIN9_4MF",
        "colab_type": "code",
        "outputId": "898f1e7c-9861-4483-a103-4e70f42919a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(len(tokens)):\n",
        "  print(tokens[i],'-->',lemma_token[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In --> In\n",
            "the --> the\n",
            "early --> early\n",
            "days --> day\n",
            ", --> ,\n",
            "many --> many\n",
            "language-processing --> language-processing\n",
            "systems --> system\n",
            "were --> were\n",
            "designed --> designed\n",
            "by --> by\n",
            "hand- --> hand-\n",
            "coding --> coding\n",
            "a --> a\n",
            "set --> set\n",
            "of --> of\n",
            "rules --> rule\n",
            ", --> ,\n",
            "[ --> [\n",
            "9 --> 9\n",
            "] --> ]\n",
            ", --> ,\n",
            "[ --> [\n",
            "10 --> 10\n",
            "] --> ]\n",
            "e.g --> e.g\n",
            ". --> .\n",
            "by --> by\n",
            "writing --> writing\n",
            "grammars --> grammar\n",
            "or --> or\n",
            "devising --> devising\n",
            "heuristic --> heuristic\n",
            "rules --> rule\n",
            "for --> for\n",
            "stemming --> stemming\n",
            ". --> .\n",
            "However --> However\n",
            ", --> ,\n",
            "this --> this\n",
            "is --> is\n",
            "rarely --> rarely\n",
            "robust --> robust\n",
            "to --> to\n",
            "natural --> natural\n",
            "language --> language\n",
            "variation --> variation\n",
            ". --> .\n",
            "Since --> Since\n",
            "the --> the\n",
            "so-called --> so-called\n",
            "`` --> ``\n",
            "statistical --> statistical\n",
            "revolution --> revolution\n",
            "'' --> ''\n",
            "[ --> [\n",
            "11 --> 11\n",
            "] --> ]\n",
            "[ --> [\n",
            "12 --> 12\n",
            "] --> ]\n",
            "in --> in\n",
            "the --> the\n",
            "late --> late\n",
            "1980s --> 1980s\n",
            "and --> and\n",
            "mid --> mid\n",
            "1990s --> 1990s\n",
            ", --> ,\n",
            "much --> much\n",
            "natural --> natural\n",
            "language --> language\n",
            "processing --> processing\n",
            "research --> research\n",
            "has --> ha\n",
            "relied --> relied\n",
            "heavily --> heavily\n",
            "on --> on\n",
            "machine --> machine\n",
            "learning.The --> learning.The\n",
            "machine-learning --> machine-learning\n",
            "paradigm --> paradigm\n",
            "calls --> call\n",
            "instead --> instead\n",
            "for --> for\n",
            "using --> using\n",
            "statistical --> statistical\n",
            "inference --> inference\n",
            "to --> to\n",
            "automatically --> automatically\n",
            "learn --> learn\n",
            "such --> such\n",
            "rules --> rule\n",
            "through --> through\n",
            "the --> the\n",
            "analysis --> analysis\n",
            "of --> of\n",
            "large --> large\n",
            "corpora --> corpus\n",
            "of --> of\n",
            "typical --> typical\n",
            "real-world --> real-world\n",
            "examples --> example\n",
            "( --> (\n",
            "a --> a\n",
            "corpus --> corpus\n",
            "( --> (\n",
            "plural --> plural\n",
            ", --> ,\n",
            "`` --> ``\n",
            "corpora --> corpus\n",
            "'' --> ''\n",
            ") --> )\n",
            "is --> is\n",
            "a --> a\n",
            "set --> set\n",
            "of --> of\n",
            "documents --> document\n",
            ", --> ,\n",
            "possibly --> possibly\n",
            "with --> with\n",
            "human --> human\n",
            "or --> or\n",
            "computer --> computer\n",
            "annotations --> annotation\n",
            ") --> )\n",
            ".Many --> .Many\n",
            "different --> different\n",
            "classes --> class\n",
            "of --> of\n",
            "machine-learning --> machine-learning\n",
            "algorithms --> algorithm\n",
            "have --> have\n",
            "been --> been\n",
            "applied --> applied\n",
            "to --> to\n",
            "natural-language-processing --> natural-language-processing\n",
            "tasks --> task\n",
            ". --> .\n",
            "These --> These\n",
            "algorithms --> algorithm\n",
            "take --> take\n",
            "as --> a\n",
            "input --> input\n",
            "a --> a\n",
            "large --> large\n",
            "set --> set\n",
            "of --> of\n",
            "`` --> ``\n",
            "features --> feature\n",
            "'' --> ''\n",
            "that --> that\n",
            "are --> are\n",
            "generated --> generated\n",
            "from --> from\n",
            "the --> the\n",
            "input --> input\n",
            "data --> data\n",
            ". --> .\n",
            "Some --> Some\n",
            "of --> of\n",
            "the --> the\n",
            "earliest-used --> earliest-used\n",
            "algorithms --> algorithm\n",
            ", --> ,\n",
            "such --> such\n",
            "as --> a\n",
            "decision --> decision\n",
            "trees --> tree\n",
            ", --> ,\n",
            "produced --> produced\n",
            "systems --> system\n",
            "of --> of\n",
            "hard --> hard\n",
            "if-then --> if-then\n",
            "rules --> rule\n",
            "similar --> similar\n",
            "to --> to\n",
            "the --> the\n",
            "systems --> system\n",
            "of --> of\n",
            "hand-written --> hand-written\n",
            "rules --> rule\n",
            "that --> that\n",
            "were --> were\n",
            "then --> then\n",
            "common --> common\n",
            ". --> .\n",
            "Increasingly --> Increasingly\n",
            ", --> ,\n",
            "however --> however\n",
            ", --> ,\n",
            "research --> research\n",
            "has --> ha\n",
            "focused --> focused\n",
            "on --> on\n",
            "statistical --> statistical\n",
            "models --> model\n",
            ", --> ,\n",
            "which --> which\n",
            "make --> make\n",
            "soft --> soft\n",
            ", --> ,\n",
            "probabilistic --> probabilistic\n",
            "decisions --> decision\n",
            "based --> based\n",
            "on --> on\n",
            "attaching --> attaching\n",
            "real-valued --> real-valued\n",
            "weights --> weight\n",
            "to --> to\n",
            "each --> each\n",
            "input --> input\n",
            "feature --> feature\n",
            ". --> .\n",
            "Such --> Such\n",
            "models --> model\n",
            "have --> have\n",
            "the --> the\n",
            "advantage --> advantage\n",
            "that --> that\n",
            "they --> they\n",
            "can --> can\n",
            "express --> express\n",
            "the --> the\n",
            "relative --> relative\n",
            "certainty --> certainty\n",
            "of --> of\n",
            "many --> many\n",
            "different --> different\n",
            "possible --> possible\n",
            "answers --> answer\n",
            "rather --> rather\n",
            "than --> than\n",
            "only --> only\n",
            "one --> one\n",
            ", --> ,\n",
            "producing --> producing\n",
            "more --> more\n",
            "reliable --> reliable\n",
            "results --> result\n",
            "when --> when\n",
            "such --> such\n",
            "a --> a\n",
            "model --> model\n",
            "is --> is\n",
            "included --> included\n",
            "as --> a\n",
            "a --> a\n",
            "component --> component\n",
            "of --> of\n",
            "a --> a\n",
            "larger --> larger\n",
            "system --> system\n",
            ". --> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QVmmyuUtUM8",
        "colab_type": "code",
        "outputId": "d4d9fdac-6fa1-49fb-8930-ffe151e97c37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# lemmatization\n",
        "# converts to base word. \n",
        "# with context\n",
        "\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemma_token=[]\n",
        "for token in tokens:\n",
        "    lemma_token.append(lemmatizer.lemmatize(token,pos=\"v\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP59kowwtICu",
        "colab_type": "code",
        "outputId": "fcd40460-2bb9-46f9-f7f6-1acedafa7e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(len(tokens)):\n",
        "  print(tokens[i],'-->',lemma_token[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In --> In\n",
            "the --> the\n",
            "early --> early\n",
            "days --> days\n",
            ", --> ,\n",
            "many --> many\n",
            "language-processing --> language-processing\n",
            "systems --> systems\n",
            "were --> be\n",
            "designed --> design\n",
            "by --> by\n",
            "hand- --> hand-\n",
            "coding --> cod\n",
            "a --> a\n",
            "set --> set\n",
            "of --> of\n",
            "rules --> rule\n",
            ", --> ,\n",
            "[ --> [\n",
            "9 --> 9\n",
            "] --> ]\n",
            ", --> ,\n",
            "[ --> [\n",
            "10 --> 10\n",
            "] --> ]\n",
            "e.g --> e.g\n",
            ". --> .\n",
            "by --> by\n",
            "writing --> write\n",
            "grammars --> grammars\n",
            "or --> or\n",
            "devising --> devise\n",
            "heuristic --> heuristic\n",
            "rules --> rule\n",
            "for --> for\n",
            "stemming --> stem\n",
            ". --> .\n",
            "However --> However\n",
            ", --> ,\n",
            "this --> this\n",
            "is --> be\n",
            "rarely --> rarely\n",
            "robust --> robust\n",
            "to --> to\n",
            "natural --> natural\n",
            "language --> language\n",
            "variation --> variation\n",
            ". --> .\n",
            "Since --> Since\n",
            "the --> the\n",
            "so-called --> so-called\n",
            "`` --> ``\n",
            "statistical --> statistical\n",
            "revolution --> revolution\n",
            "'' --> ''\n",
            "[ --> [\n",
            "11 --> 11\n",
            "] --> ]\n",
            "[ --> [\n",
            "12 --> 12\n",
            "] --> ]\n",
            "in --> in\n",
            "the --> the\n",
            "late --> late\n",
            "1980s --> 1980s\n",
            "and --> and\n",
            "mid --> mid\n",
            "1990s --> 1990s\n",
            ", --> ,\n",
            "much --> much\n",
            "natural --> natural\n",
            "language --> language\n",
            "processing --> process\n",
            "research --> research\n",
            "has --> have\n",
            "relied --> rely\n",
            "heavily --> heavily\n",
            "on --> on\n",
            "machine --> machine\n",
            "learning.The --> learning.The\n",
            "machine-learning --> machine-learning\n",
            "paradigm --> paradigm\n",
            "calls --> call\n",
            "instead --> instead\n",
            "for --> for\n",
            "using --> use\n",
            "statistical --> statistical\n",
            "inference --> inference\n",
            "to --> to\n",
            "automatically --> automatically\n",
            "learn --> learn\n",
            "such --> such\n",
            "rules --> rule\n",
            "through --> through\n",
            "the --> the\n",
            "analysis --> analysis\n",
            "of --> of\n",
            "large --> large\n",
            "corpora --> corpora\n",
            "of --> of\n",
            "typical --> typical\n",
            "real-world --> real-world\n",
            "examples --> examples\n",
            "( --> (\n",
            "a --> a\n",
            "corpus --> corpus\n",
            "( --> (\n",
            "plural --> plural\n",
            ", --> ,\n",
            "`` --> ``\n",
            "corpora --> corpora\n",
            "'' --> ''\n",
            ") --> )\n",
            "is --> be\n",
            "a --> a\n",
            "set --> set\n",
            "of --> of\n",
            "documents --> document\n",
            ", --> ,\n",
            "possibly --> possibly\n",
            "with --> with\n",
            "human --> human\n",
            "or --> or\n",
            "computer --> computer\n",
            "annotations --> annotations\n",
            ") --> )\n",
            ".Many --> .Many\n",
            "different --> different\n",
            "classes --> class\n",
            "of --> of\n",
            "machine-learning --> machine-learning\n",
            "algorithms --> algorithms\n",
            "have --> have\n",
            "been --> be\n",
            "applied --> apply\n",
            "to --> to\n",
            "natural-language-processing --> natural-language-processing\n",
            "tasks --> task\n",
            ". --> .\n",
            "These --> These\n",
            "algorithms --> algorithms\n",
            "take --> take\n",
            "as --> as\n",
            "input --> input\n",
            "a --> a\n",
            "large --> large\n",
            "set --> set\n",
            "of --> of\n",
            "`` --> ``\n",
            "features --> feature\n",
            "'' --> ''\n",
            "that --> that\n",
            "are --> be\n",
            "generated --> generate\n",
            "from --> from\n",
            "the --> the\n",
            "input --> input\n",
            "data --> data\n",
            ". --> .\n",
            "Some --> Some\n",
            "of --> of\n",
            "the --> the\n",
            "earliest-used --> earliest-used\n",
            "algorithms --> algorithms\n",
            ", --> ,\n",
            "such --> such\n",
            "as --> as\n",
            "decision --> decision\n",
            "trees --> tree\n",
            ", --> ,\n",
            "produced --> produce\n",
            "systems --> systems\n",
            "of --> of\n",
            "hard --> hard\n",
            "if-then --> if-then\n",
            "rules --> rule\n",
            "similar --> similar\n",
            "to --> to\n",
            "the --> the\n",
            "systems --> systems\n",
            "of --> of\n",
            "hand-written --> hand-written\n",
            "rules --> rule\n",
            "that --> that\n",
            "were --> be\n",
            "then --> then\n",
            "common --> common\n",
            ". --> .\n",
            "Increasingly --> Increasingly\n",
            ", --> ,\n",
            "however --> however\n",
            ", --> ,\n",
            "research --> research\n",
            "has --> have\n",
            "focused --> focus\n",
            "on --> on\n",
            "statistical --> statistical\n",
            "models --> model\n",
            ", --> ,\n",
            "which --> which\n",
            "make --> make\n",
            "soft --> soft\n",
            ", --> ,\n",
            "probabilistic --> probabilistic\n",
            "decisions --> decisions\n",
            "based --> base\n",
            "on --> on\n",
            "attaching --> attach\n",
            "real-valued --> real-valued\n",
            "weights --> weight\n",
            "to --> to\n",
            "each --> each\n",
            "input --> input\n",
            "feature --> feature\n",
            ". --> .\n",
            "Such --> Such\n",
            "models --> model\n",
            "have --> have\n",
            "the --> the\n",
            "advantage --> advantage\n",
            "that --> that\n",
            "they --> they\n",
            "can --> can\n",
            "express --> express\n",
            "the --> the\n",
            "relative --> relative\n",
            "certainty --> certainty\n",
            "of --> of\n",
            "many --> many\n",
            "different --> different\n",
            "possible --> possible\n",
            "answers --> answer\n",
            "rather --> rather\n",
            "than --> than\n",
            "only --> only\n",
            "one --> one\n",
            ", --> ,\n",
            "producing --> produce\n",
            "more --> more\n",
            "reliable --> reliable\n",
            "results --> result\n",
            "when --> when\n",
            "such --> such\n",
            "a --> a\n",
            "model --> model\n",
            "is --> be\n",
            "included --> include\n",
            "as --> as\n",
            "a --> a\n",
            "component --> component\n",
            "of --> of\n",
            "a --> a\n",
            "larger --> larger\n",
            "system --> system\n",
            ". --> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne4aOKuXM9Ec",
        "colab_type": "text"
      },
      "source": [
        "# Frequency distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhkn5UosA5sC",
        "colab_type": "code",
        "outputId": "1cb3f49a-0807-4e3c-a04c-796b1acd2809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# frequency distribution of tokens\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "sorted(FreqDist(tokens).items(),key=lambda k:k[1], reverse=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 14),\n",
              " ('of', 11),\n",
              " ('the', 9),\n",
              " ('.', 8),\n",
              " ('a', 7),\n",
              " ('rules', 5),\n",
              " ('to', 5),\n",
              " ('[', 4),\n",
              " (']', 4),\n",
              " ('systems', 3),\n",
              " ('set', 3),\n",
              " ('is', 3),\n",
              " ('``', 3),\n",
              " ('statistical', 3),\n",
              " (\"''\", 3),\n",
              " ('on', 3),\n",
              " ('such', 3),\n",
              " ('algorithms', 3),\n",
              " ('as', 3),\n",
              " ('input', 3),\n",
              " ('that', 3),\n",
              " ('many', 2),\n",
              " ('were', 2),\n",
              " ('by', 2),\n",
              " ('or', 2),\n",
              " ('for', 2),\n",
              " ('natural', 2),\n",
              " ('language', 2),\n",
              " ('research', 2),\n",
              " ('has', 2),\n",
              " ('machine-learning', 2),\n",
              " ('large', 2),\n",
              " ('corpora', 2),\n",
              " ('(', 2),\n",
              " (')', 2),\n",
              " ('different', 2),\n",
              " ('have', 2),\n",
              " ('models', 2),\n",
              " ('In', 1),\n",
              " ('early', 1),\n",
              " ('days', 1),\n",
              " ('language-processing', 1),\n",
              " ('designed', 1),\n",
              " ('hand-', 1),\n",
              " ('coding', 1),\n",
              " ('9', 1),\n",
              " ('10', 1),\n",
              " ('e.g', 1),\n",
              " ('writing', 1),\n",
              " ('grammars', 1),\n",
              " ('devising', 1),\n",
              " ('heuristic', 1),\n",
              " ('stemming', 1),\n",
              " ('However', 1),\n",
              " ('this', 1),\n",
              " ('rarely', 1),\n",
              " ('robust', 1),\n",
              " ('variation', 1),\n",
              " ('Since', 1),\n",
              " ('so-called', 1),\n",
              " ('revolution', 1),\n",
              " ('11', 1),\n",
              " ('12', 1),\n",
              " ('in', 1),\n",
              " ('late', 1),\n",
              " ('1980s', 1),\n",
              " ('and', 1),\n",
              " ('mid', 1),\n",
              " ('1990s', 1),\n",
              " ('much', 1),\n",
              " ('processing', 1),\n",
              " ('relied', 1),\n",
              " ('heavily', 1),\n",
              " ('machine', 1),\n",
              " ('learning.The', 1),\n",
              " ('paradigm', 1),\n",
              " ('calls', 1),\n",
              " ('instead', 1),\n",
              " ('using', 1),\n",
              " ('inference', 1),\n",
              " ('automatically', 1),\n",
              " ('learn', 1),\n",
              " ('through', 1),\n",
              " ('analysis', 1),\n",
              " ('typical', 1),\n",
              " ('real-world', 1),\n",
              " ('examples', 1),\n",
              " ('corpus', 1),\n",
              " ('plural', 1),\n",
              " ('documents', 1),\n",
              " ('possibly', 1),\n",
              " ('with', 1),\n",
              " ('human', 1),\n",
              " ('computer', 1),\n",
              " ('annotations', 1),\n",
              " ('.Many', 1),\n",
              " ('classes', 1),\n",
              " ('been', 1),\n",
              " ('applied', 1),\n",
              " ('natural-language-processing', 1),\n",
              " ('tasks', 1),\n",
              " ('These', 1),\n",
              " ('take', 1),\n",
              " ('features', 1),\n",
              " ('are', 1),\n",
              " ('generated', 1),\n",
              " ('from', 1),\n",
              " ('data', 1),\n",
              " ('Some', 1),\n",
              " ('earliest-used', 1),\n",
              " ('decision', 1),\n",
              " ('trees', 1),\n",
              " ('produced', 1),\n",
              " ('hard', 1),\n",
              " ('if-then', 1),\n",
              " ('similar', 1),\n",
              " ('hand-written', 1),\n",
              " ('then', 1),\n",
              " ('common', 1),\n",
              " ('Increasingly', 1),\n",
              " ('however', 1),\n",
              " ('focused', 1),\n",
              " ('which', 1),\n",
              " ('make', 1),\n",
              " ('soft', 1),\n",
              " ('probabilistic', 1),\n",
              " ('decisions', 1),\n",
              " ('based', 1),\n",
              " ('attaching', 1),\n",
              " ('real-valued', 1),\n",
              " ('weights', 1),\n",
              " ('each', 1),\n",
              " ('feature', 1),\n",
              " ('Such', 1),\n",
              " ('advantage', 1),\n",
              " ('they', 1),\n",
              " ('can', 1),\n",
              " ('express', 1),\n",
              " ('relative', 1),\n",
              " ('certainty', 1),\n",
              " ('possible', 1),\n",
              " ('answers', 1),\n",
              " ('rather', 1),\n",
              " ('than', 1),\n",
              " ('only', 1),\n",
              " ('one', 1),\n",
              " ('producing', 1),\n",
              " ('more', 1),\n",
              " ('reliable', 1),\n",
              " ('results', 1),\n",
              " ('when', 1),\n",
              " ('model', 1),\n",
              " ('included', 1),\n",
              " ('component', 1),\n",
              " ('larger', 1),\n",
              " ('system', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO_nmE8DDL-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}